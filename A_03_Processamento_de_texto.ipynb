{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsc86/Processamento-de-Linguagem-Natural---PLN/blob/main/Aula_03_Processamento_de_texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 - Normaliza√ß√£o do texto e remo√ß√£o de ru√≠do\n"
      ],
      "metadata": {
        "id": "APSOjzOoqCOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa biblioteca para express√µes regulares (limpeza de texto)\n",
        "import re\n",
        "\n",
        "# Texto original com ru√≠dos (pontua√ß√µes, s√≠mbolos, varia√ß√£o de caixa)\n",
        "original = \"Ol√°!!! Este √© um exemplo de texto, com v√°rias PONTUA√á√ïES, s√≠mbolos #especiais, e LETRAS mai√∫sculas.\"\n",
        "\n",
        "# Remove caracteres especiais (mant√©m apenas letras e espa√ßos)\n",
        "texto_limpo = re.sub(r'[^A-Za-z√Ä-√ø\\s]', '', original)\n",
        "\n",
        "# Padroniza texto em min√∫sculas\n",
        "texto_normalizado = texto_limpo.lower()\n",
        "\n",
        "# Exibe resultados do processamento\n",
        "print('Texto original:', original)\n",
        "print('Texto limpo:', texto_limpo)\n",
        "print('Texto normalizado:', texto_normalizado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-caxqYdofC1",
        "outputId": "2a1d0a56-39b4-4f2e-fb8c-ad5cf3359784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original: Ol√°!!! Este √© um exemplo de texto, com v√°rias PONTUA√á√ïES, s√≠mbolos #especiais, e LETRAS mai√∫sculas.\n",
            "Texto limpo: Ol√° Este √© um exemplo de texto com v√°rias PONTUA√á√ïES s√≠mbolos especiais e LETRAS mai√∫sculas\n",
            "Texto normalizado: ol√° este √© um exemplo de texto com v√°rias pontua√ß√µes s√≠mbolos especiais e letras mai√∫sculas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 - Tokeniza√ß√£o\n"
      ],
      "metadata": {
        "id": "HsOrunLUqqL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa biblioteca NLP e tokenizador\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Baixa modelo de tokeniza√ß√£o para portugu√™s\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Exibe textos processados (original, limpo e normalizado)\n",
        "print(f'Texto original: {original}')\n",
        "print(f'\\nTexto limpo: {texto_limpo}')\n",
        "print(f'\\nTexto normalizado: {texto_normalizado}')\n",
        "\n",
        "# Tokeniza texto normalizado (divide em palavras individuais)\n",
        "tokens = word_tokenize(texto_normalizado, language='portuguese')\n",
        "print(f'\\nTokens extra√≠dos: {tokens}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIgsN3msqxAf",
        "outputId": "4af7ed1d-1949-4f51-9d8d-0df1bd9193d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original: Ol√°!!! Este √© um exemplo de texto, com v√°rias PONTUA√á√ïES, s√≠mbolos #especiais, e LETRAS mai√∫sculas.\n",
            "\n",
            "Texto limpo: Ol√° Este √© um exemplo de texto com v√°rias PONTUA√á√ïES s√≠mbolos especiais e LETRAS mai√∫sculas\n",
            "\n",
            "Texto normalizado: ol√° este √© um exemplo de texto com v√°rias pontua√ß√µes s√≠mbolos especiais e letras mai√∫sculas\n",
            "\n",
            "Tokens extra√≠dos: ['ol√°', 'este', '√©', 'um', 'exemplo', 'de', 'texto', 'com', 'v√°rias', 'pontua√ß√µes', 's√≠mbolos', 'especiais', 'e', 'letras', 'mai√∫sculas']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 - Remo√ß√£o de Stepwords\n"
      ],
      "metadata": {
        "id": "0XkaniIhrmjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa lista de stopwords (palavras sem significado contextual)\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Baixa lista de stopwords para portugu√™s\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Carrega stopwords em conjunto para busca eficiente\n",
        "stopwords_pt = set(stopwords.words('portuguese'))\n",
        "print(\"Stopwords em portugu√™s:\", stopwords_pt)\n",
        "\n",
        "# Filtra tokens removendo stopwords (palavras muito comuns)\n",
        "tokens_sem_stopwords = [palavra for palavra in tokens if palavra.lower() not in stopwords_pt]\n",
        "\n",
        "# Exibe resultados comparativos\n",
        "print('\\nTokens originais:', tokens, '\\nQuantidade:', len(tokens))\n",
        "print('\\nTokens sem stopwords:', tokens_sem_stopwords, '\\nQuantidade:', len(tokens_sem_stopwords))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aC26o5KirtpN",
        "outputId": "404c2229-b603-48ea-9b95-b14b1d79b175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopwords em portugu√™s: {'hajam', 'seus', 's√≥', 'num', 'nem', 'n√≥s', 'esta', 'da', 'nas', 'houv√©ssemos', 'mesmo', 't√≠nhamos', 'dele', 'seria', 'ao', 'forem', 'serei', 'por', 'for', 'eles', 'seu', '√†s', 'houverem', 'tiv√©ssemos', 'houvesse', 'fomos', 'estas', '√©', 'das', 'houver', 'fui', 'haver', 'mais', 'quem', 'essa', 'nossa', 'estiveram', 'formos', 'quando', 'depois', 'n√£o', 'estiver', 'sua', 'sejam', 'estivesse', 'seriam', 'se', 'seja', 'sejamos', 'entre', 'de', 'estamos', 'estiverem', 'pelas', 'terei', 'este', 'haja', 'j√°', 'aos', 'eram', 'suas', 'sem', 'tenham', 'teve', 'houver√≠amos', 'tivessem', 'nossos', 'tua', 'tenho', 'era', 'houvessem', 'tivesse', 'vos', 'est√°vamos', 'tu', 'foi', 'estiv√©ramos', 'houveram', 'no', 'o', 'eu', 'esses', 'ela', 'estava', 'dela', 'te', 'as', 'estes', 'mas', 'qual', 'tive', 'houve', 'h√£o', 'estejamos', 'at√©', 'nos', 'teria', 'tivera', 'meus', 'houvera', 'aquilo', 'foram', 'houverei', 't√©m', 'havemos', 'houveremos', 'meu', 'ser', 'tiverem', 'houv√©ramos', 'ele', 'isto', 'estivermos', 'um', 'estiv√©ssemos', 'houvemos', 'tiveram', 'ter√°', 'ter√£o', 'minhas', 'voc√™', 'esteja', 'estejam', 'temos', 'ter√≠amos', 'deles', 'esse', 'hajamos', 'que', 'fossem', 'est√£o', 'houvermos', 'estavam', 'f√¥ramos', 'e', 'teus', 'nosso', 'pelos', 'lhe', 's√£o', 'voc√™s', 'esteve', 'uma', 'teriam', 'f√¥ssemos', 'teremos', 'delas', 'h√°', 'tinha', 'estar', 'sou', 'tinham', 'estivemos', 'os', 'ser√£o', 'do', '√†', 'nossas', '√©ramos', 'ser√°', 'tenha', 'essas', 'dos', 'em', 'muito', 'tivermos', 'tivemos', 'me', 'na', 'aqueles', 'isso', 'aquela', 'seremos', 'tuas', 'estou', 'hei', 'estivessem', 'tem', 'fosse', 'aquelas', 'tiv√©ramos', 'para', 'tenhamos', 'houver√£o', 'pela', 'elas', 'houver√°', 'houveriam', 'minha', 'estivera', 'numa', 'ou', 'tamb√©m', 'teu', 'como', 'lhes', 'somos', 'pelo', 'a', 'estive', 'com', 'houveria', 'tiver', 'est√°', 'aquele', 'fora', 'ser√≠amos'}\n",
            "\n",
            "Tokens originais: ['ol√°', 'este', '√©', 'um', 'exemplo', 'de', 'texto', 'com', 'v√°rias', 'pontua√ß√µes', 's√≠mbolos', 'especiais', 'e', 'letras', 'mai√∫sculas'] \n",
            "Quantidade: 15\n",
            "\n",
            "Tokens sem stopwords: ['ol√°', 'exemplo', 'texto', 'v√°rias', 'pontua√ß√µes', 's√≠mbolos', 'especiais', 'letras', 'mai√∫sculas'] \n",
            "Quantidade: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 - Stemming e Lemaliza√ß√£o"
      ],
      "metadata": {
        "id": "XfKaYWIssOTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Importa stemmer para redu√ß√£o de palavras ao radical\n",
        "from nltk.stem import RSLPStemmer\n",
        "\n",
        "# Baixa recursos para stemmer em portugu√™s\n",
        "nltk.download('rslp')\n",
        "\n",
        "# Cria inst√¢ncia do stemmer RSLP (espec√≠fico para portugu√™s)\n",
        "stemmer = RSLPStemmer()\n",
        "\n",
        "# Aplica stemming em cada palavra (reduz √† forma radical)\n",
        "stemming = [stemmer.stem(palavra) for palavra in tokens_sem_stopwords]\n",
        "\n",
        "# Exibe compara√ß√£o entre tokens originais e seus radicais\n",
        "print('Tokens originais:', tokens_sem_stopwords)\n",
        "print('Tokens ap√≥s stemming:', stemming)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdNq_wXxs64G",
        "outputId": "b237914b-2fe9-4a67-93dd-71f85d585c60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens originais: ['ol√°', 'exemplo', 'texto', 'v√°rias', 'pontua√ß√µes', 's√≠mbolos', 'especiais', 'letras', 'mai√∫sculas']\n",
            "Tokens ap√≥s stemming: ['ol√°', 'exempl', 'text', 'v√°r', 'pontu', 's√≠mbol', 'espec', 'letr', 'mai√∫scul']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Exemplo 01 - Pr√© processamento completo\n"
      ],
      "metadata": {
        "id": "eafAzeIhtr2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa bibliotecas para processamento de linguagem natural\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import RSLPStemmer  # Stemmer espec√≠fico para portugu√™s\n",
        "import re\n",
        "\n",
        "# Baixa recursos necess√°rios (executar apenas na primeira vez)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('rslp')\n",
        "\n",
        "# Recebe texto do usu√°rio\n",
        "texto = input(\"Digite seu texto (pode conter s√≠mbolos): \")\n",
        "\n",
        "# 1. LIMPEZA DO TEXTO\n",
        "# Remove caracteres n√£o alfab√©ticos e normaliza espa√ßos\n",
        "texto_limpo = re.sub(r'[^a-zA-Z√°-√∫√Å-√ö]', ' ', texto)\n",
        "texto_limpo = ' '.join(texto_limpo.split())  # Remove espa√ßos extras\n",
        "\n",
        "# 2. NORMALIZA√á√ÉO\n",
        "# Converte para min√∫sculas\n",
        "texto_normalizado = texto_limpo.lower()\n",
        "\n",
        "# 3. TOKENIZA√á√ÉO\n",
        "# Divide o texto em palavras individuais\n",
        "tokens = nltk.word_tokenize(texto_normalizado, language='portuguese')\n",
        "\n",
        "# 4. REMO√á√ÉO DE STOPWORDS\n",
        "# Filtra palavras sem significado contextual\n",
        "stopwords_pt = set(stopwords.words('portuguese'))\n",
        "tokens_sem_stopwords = [p for p in tokens if p not in stopwords_pt]\n",
        "\n",
        "# 5. STEMMING (REDU√á√ÉO AO RADICAL)\n",
        "# Aplica stemmer espec√≠fico para portugu√™s\n",
        "stemmer = RSLPStemmer()\n",
        "radicais = [stemmer.stem(palavra) for palavra in tokens_sem_stopwords]\n",
        "\n",
        "# RESULTADOS\n",
        "print(\"\\nProcessamento completo:\")\n",
        "print(f\"Texto original: {texto}\")\n",
        "print(f\"Texto limpo: {texto_limpo}\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"Sem stopwords: {tokens_sem_stopwords}\")\n",
        "print(f\"Radicais: {radicais}\")\n",
        "print(f\"\\nQuantidade de palavras: {len(radicais)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3LqS7Wktwk_",
        "outputId": "4a41edf3-01c3-472b-9565-d4df5ade84ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digite seu texto (pode conter s√≠mbolos): oi\n",
            "\n",
            "Processamento completo:\n",
            "Texto original: oi\n",
            "Texto limpo: oi\n",
            "Tokens: ['oi']\n",
            "Sem stopwords: ['oi']\n",
            "Radicais: ['oi']\n",
            "\n",
            "Quantidade de palavras: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exemplo 02 - Estrutura e pr√© processamento de texto"
      ],
      "metadata": {
        "id": "TINoPxtRuRsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa√ß√µes otimizadas para PLN\n",
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Verifica√ß√£o e download de recursos necess√°rios\n",
        "try:\n",
        "    nlp = spacy.load(\"pt_core_news_sm\")\n",
        "except OSError:\n",
        "    print(\"Baixando modelo do spaCy...\")\n",
        "    !python -m spacy download pt_core_news_sm\n",
        "    nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "nltk.download(['stopwords', 'punkt_tab', 'rslp'], quiet=True)\n",
        "\n",
        "# Texto de exemplo para an√°lise\n",
        "texto = \"\"\"\n",
        "O Processamento de Linguagem Natural (PLN) revoluciona como interagimos com tecnologia.\n",
        "An√°lise de sentimentos, chatbots e tradutores autom√°ticos s√£o aplica√ß√µes comuns.\n",
        "Veja exemplos em: https://exemplo-pln.com.br! üòä\n",
        "\"\"\"\n",
        "\n",
        "# Pipeline de processamento\n",
        "def processar_texto(texto):\n",
        "    # 1. Normaliza√ß√£o\n",
        "    texto = re.sub(r'https?://\\S+|www\\.\\S+', '', texto)  # Remove URLs\n",
        "    texto = re.sub(r'[^\\w\\s√°-√∫√Å-√ö]', '', texto.lower())  # Padroniza√ß√£o\n",
        "\n",
        "    # 2. Tokeniza√ß√£o\n",
        "    tokens = nltk.word_tokenize(texto, language='portuguese')\n",
        "\n",
        "    # 3. Filtragem\n",
        "    stopwords_pt = set(stopwords.words('portuguese'))\n",
        "    tokens_filtrados = [t for t in tokens if t not in stopwords_pt and len(t) > 2]\n",
        "\n",
        "    # 4. Stemming\n",
        "    stemmer = nltk.RSLPStemmer()\n",
        "    stems = [stemmer.stem(t) for t in tokens_filtrados]\n",
        "\n",
        "    # 5. Lematiza√ß√£o\n",
        "    lemas = [t.lemma_ for t in nlp(\" \".join(tokens_filtrados))]\n",
        "\n",
        "    return {\n",
        "        'original': texto,\n",
        "        'tokens': tokens,\n",
        "        'filtrados': tokens_filtrados,\n",
        "        'stems': stems,\n",
        "        'lemas': lemas\n",
        "    }\n",
        "\n",
        "# Processamento e exibi√ß√£o\n",
        "resultado = processar_texto(texto)\n",
        "print(f\"\\nTexto original:\\n{texto}\")\n",
        "print(f\"\\nTokens ({len(resultado['tokens'])}):\\n{resultado['tokens']}\")\n",
        "print(f\"\\nSem stopwords ({len(resultado['filtrados'])}):\\n{resultado['filtrados']}\")\n",
        "print(f\"\\nStems ({len(resultado['stems'])}):\\n{resultado['stems']}\")\n",
        "print(f\"\\nLematiza√ß√£o ({len(resultado['lemas'])}):\\n{resultado['lemas']}\")\n",
        "\n",
        "# An√°lise comparativa\n",
        "diferencas = sum(1 for s,l in zip(resultado['stems'], resultado['lemas']) if s != l)\n",
        "print(f\"\\nDiferen√ßas entre stemming/lematiza√ß√£o: {diferencas}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BPaWgS0uYAT",
        "outputId": "110bc3f9-4d02-4227-d1da-5f2079bad2fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baixando modelo do spaCy...\n",
            "Collecting pt-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pt-core-news-sm\n",
            "Successfully installed pt-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "\n",
            "Texto original:\n",
            "\n",
            "O Processamento de Linguagem Natural (PLN) revoluciona como interagimos com tecnologia.\n",
            "An√°lise de sentimentos, chatbots e tradutores autom√°ticos s√£o aplica√ß√µes comuns. \n",
            "Veja exemplos em: https://exemplo-pln.com.br! üòä\n",
            "\n",
            "\n",
            "Tokens (24):\n",
            "['o', 'processamento', 'de', 'linguagem', 'natural', 'pln', 'revoluciona', 'como', 'interagimos', 'com', 'tecnologia', 'an√°lise', 'de', 'sentimentos', 'chatbots', 'e', 'tradutores', 'autom√°ticos', 's√£o', 'aplica√ß√µes', 'comuns', 'veja', 'exemplos', 'em']\n",
            "\n",
            "Sem stopwords (16):\n",
            "['processamento', 'linguagem', 'natural', 'pln', 'revoluciona', 'interagimos', 'tecnologia', 'an√°lise', 'sentimentos', 'chatbots', 'tradutores', 'autom√°ticos', 'aplica√ß√µes', 'comuns', 'veja', 'exemplos']\n",
            "\n",
            "Stems (16):\n",
            "['process', 'lingu', 'natur', 'pln', 'revoluc', 'interag', 'tecnolog', 'an√°lis', 'sent', 'chatbot', 'tradu', 'autom√°', 'aplic', 'comum', 'vej', 'exempl']\n",
            "\n",
            "Lematiza√ß√£o (16):\n",
            "['processamento', 'linguagem', 'natural', 'pln', 'revolucionar', 'interagimos', 'tecnologia', 'an√°liser', 'sentimento', 'chatbots', 'tradutor', 'autom√°tico', 'aplica√ß√£o', 'comum', 'ver', 'exemplo']\n",
            "\n",
            "Diferen√ßas entre stemming/lematiza√ß√£o: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exemplo 03 - Modelo pr√© treinado\n"
      ],
      "metadata": {
        "id": "txmhzCC1-MRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Importa a biblioteca spaCy para processamento de linguagem natural\n",
        "import spacy\n",
        "from spacy import displacy  # Para visualiza√ß√£o de depend√™ncias\n",
        "\n",
        "# Carrega o modelo pr√©-treinado para portugu√™s\n",
        "# (Certifique-se de ter executado antes: !python -m spacy download pt_core_news_sm)\n",
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "# Recebe texto do usu√°rio para an√°lise\n",
        "texto = input(\"Digite um texto para an√°lise lingu√≠stica: \")\n",
        "\n",
        "# Processa o texto com o pipeline do spaCy\n",
        "doc = nlp(texto)\n",
        "\n",
        "# 1. AN√ÅLISE MORFOSSINT√ÅTICA (Part-of-Speech Tagging)\n",
        "print('\\nüî† AN√ÅLISE GRAMATICAL:')\n",
        "for token in doc:\n",
        "    print(f\"Palavra: {token.text:<15} | Classe gramatical: {token.pos_:<10} | Tag detalhada: {token.tag_}\")\n",
        "\n",
        "# 2. AN√ÅLISE DE DEPEND√äNCIAS\n",
        "print(\"\\nüîó AN√ÅLISE DE DEPEND√äNCIAS:\")\n",
        "for token in doc:\n",
        "    print(f\"Palavra: {token.text:<15} | Rela√ß√£o: {token.dep_:<10} | Governante: {token.head.text}\")\n",
        "\n",
        "# 3. VISUALIZA√á√ÉO GR√ÅFICA (opcional - funciona no Jupyter/Colab)\n",
        "print(\"\\nüå≥ VISUALIZA√á√ÉO DA √ÅRVORE DE DEPEND√äNCIAS:\")\n",
        "displacy.render(doc, style=\"dep\", jupyter=True, options={'distance': 90})\n",
        "\n",
        "# 4. INFORMA√á√ïES ADICIONAIS\n",
        "print(\"\\nüìä ESTAT√çSTICAS DO TEXTO:\")\n",
        "print(f\"N√∫mero de tokens: {len(doc)}\")\n",
        "print(f\"N√∫mero de frases: {len(list(doc.sents))}\")\n",
        "print(f\"Palavras √∫nicas: {len(set(token.text for token in doc))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "UHpsxXo3-ekM",
        "outputId": "edb935bb-d752-4e49-f1c1-d42d372def30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digite um texto para an√°lise lingu√≠stica: ola! tudo bem?\n",
            "\n",
            "üî† AN√ÅLISE GRAMATICAL:\n",
            "Palavra: ola             | Classe gramatical: INTJ       | Tag detalhada: INTJ\n",
            "Palavra: !               | Classe gramatical: PUNCT      | Tag detalhada: PUNCT\n",
            "Palavra: tudo            | Classe gramatical: PRON       | Tag detalhada: PRON\n",
            "Palavra: bem             | Classe gramatical: ADV        | Tag detalhada: ADV\n",
            "Palavra: ?               | Classe gramatical: PUNCT      | Tag detalhada: PUNCT\n",
            "\n",
            "üîó AN√ÅLISE DE DEPEND√äNCIAS:\n",
            "Palavra: ola             | Rela√ß√£o: ROOT       | Governante: ola\n",
            "Palavra: !               | Rela√ß√£o: punct      | Governante: ola\n",
            "Palavra: tudo            | Rela√ß√£o: ROOT       | Governante: tudo\n",
            "Palavra: bem             | Rela√ß√£o: advmod     | Governante: tudo\n",
            "Palavra: ?               | Rela√ß√£o: punct      | Governante: tudo\n",
            "\n",
            "üå≥ VISUALIZA√á√ÉO DA √ÅRVORE DE DEPEND√äNCIAS:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"pt\" id=\"12e12ac27b7f4b079fbfe6a1392ad1e0-0\" class=\"displacy\" width=\"320\" height=\"182.0\" direction=\"ltr\" style=\"max-width: none; height: 182.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"92.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">ola!</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">INTJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"92.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">tudo</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"92.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">bem?</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">ADV</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-12e12ac27b7f4b079fbfe6a1392ad1e0-0-0\" stroke-width=\"2px\" d=\"M160,47.0 C160,2.0 230.0,2.0 230.0,47.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-12e12ac27b7f4b079fbfe6a1392ad1e0-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M230.0,49.0 L238.0,37.0 222.0,37.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä ESTAT√çSTICAS DO TEXTO:\n",
            "N√∫mero de tokens: 5\n",
            "N√∫mero de frases: 2\n",
            "Palavras √∫nicas: 5\n"
          ]
        }
      ]
    }
  ]
}
