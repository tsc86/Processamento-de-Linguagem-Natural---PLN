{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsc86/Processamento-de-Linguagem-Natural---PLN/blob/main/Aula_03_Processamento_de_texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 - Normalização do texto e remoção de ruído\n"
      ],
      "metadata": {
        "id": "APSOjzOoqCOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa biblioteca para expressões regulares (limpeza de texto)\n",
        "import re\n",
        "\n",
        "# Texto original com ruídos (pontuações, símbolos, variação de caixa)\n",
        "original = \"Olá!!! Este é um exemplo de texto, com várias PONTUAÇÕES, símbolos #especiais, e LETRAS maiúsculas.\"\n",
        "\n",
        "# Remove caracteres especiais (mantém apenas letras e espaços)\n",
        "texto_limpo = re.sub(r'[^A-Za-zÀ-ÿ\\s]', '', original)\n",
        "\n",
        "# Padroniza texto em minúsculas\n",
        "texto_normalizado = texto_limpo.lower()\n",
        "\n",
        "# Exibe resultados do processamento\n",
        "print('Texto original:', original)\n",
        "print('Texto limpo:', texto_limpo)\n",
        "print('Texto normalizado:', texto_normalizado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-caxqYdofC1",
        "outputId": "2a1d0a56-39b4-4f2e-fb8c-ad5cf3359784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original: Olá!!! Este é um exemplo de texto, com várias PONTUAÇÕES, símbolos #especiais, e LETRAS maiúsculas.\n",
            "Texto limpo: Olá Este é um exemplo de texto com várias PONTUAÇÕES símbolos especiais e LETRAS maiúsculas\n",
            "Texto normalizado: olá este é um exemplo de texto com várias pontuações símbolos especiais e letras maiúsculas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 - Tokenização\n"
      ],
      "metadata": {
        "id": "HsOrunLUqqL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa biblioteca NLP e tokenizador\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Baixa modelo de tokenização para português\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Exibe textos processados (original, limpo e normalizado)\n",
        "print(f'Texto original: {original}')\n",
        "print(f'\\nTexto limpo: {texto_limpo}')\n",
        "print(f'\\nTexto normalizado: {texto_normalizado}')\n",
        "\n",
        "# Tokeniza texto normalizado (divide em palavras individuais)\n",
        "tokens = word_tokenize(texto_normalizado, language='portuguese')\n",
        "print(f'\\nTokens extraídos: {tokens}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIgsN3msqxAf",
        "outputId": "4af7ed1d-1949-4f51-9d8d-0df1bd9193d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original: Olá!!! Este é um exemplo de texto, com várias PONTUAÇÕES, símbolos #especiais, e LETRAS maiúsculas.\n",
            "\n",
            "Texto limpo: Olá Este é um exemplo de texto com várias PONTUAÇÕES símbolos especiais e LETRAS maiúsculas\n",
            "\n",
            "Texto normalizado: olá este é um exemplo de texto com várias pontuações símbolos especiais e letras maiúsculas\n",
            "\n",
            "Tokens extraídos: ['olá', 'este', 'é', 'um', 'exemplo', 'de', 'texto', 'com', 'várias', 'pontuações', 'símbolos', 'especiais', 'e', 'letras', 'maiúsculas']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 - Remoção de Stepwords\n"
      ],
      "metadata": {
        "id": "0XkaniIhrmjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa lista de stopwords (palavras sem significado contextual)\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Baixa lista de stopwords para português\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Carrega stopwords em conjunto para busca eficiente\n",
        "stopwords_pt = set(stopwords.words('portuguese'))\n",
        "print(\"Stopwords em português:\", stopwords_pt)\n",
        "\n",
        "# Filtra tokens removendo stopwords (palavras muito comuns)\n",
        "tokens_sem_stopwords = [palavra for palavra in tokens if palavra.lower() not in stopwords_pt]\n",
        "\n",
        "# Exibe resultados comparativos\n",
        "print('\\nTokens originais:', tokens, '\\nQuantidade:', len(tokens))\n",
        "print('\\nTokens sem stopwords:', tokens_sem_stopwords, '\\nQuantidade:', len(tokens_sem_stopwords))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aC26o5KirtpN",
        "outputId": "404c2229-b603-48ea-9b95-b14b1d79b175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopwords em português: {'hajam', 'seus', 'só', 'num', 'nem', 'nós', 'esta', 'da', 'nas', 'houvéssemos', 'mesmo', 'tínhamos', 'dele', 'seria', 'ao', 'forem', 'serei', 'por', 'for', 'eles', 'seu', 'às', 'houverem', 'tivéssemos', 'houvesse', 'fomos', 'estas', 'é', 'das', 'houver', 'fui', 'haver', 'mais', 'quem', 'essa', 'nossa', 'estiveram', 'formos', 'quando', 'depois', 'não', 'estiver', 'sua', 'sejam', 'estivesse', 'seriam', 'se', 'seja', 'sejamos', 'entre', 'de', 'estamos', 'estiverem', 'pelas', 'terei', 'este', 'haja', 'já', 'aos', 'eram', 'suas', 'sem', 'tenham', 'teve', 'houveríamos', 'tivessem', 'nossos', 'tua', 'tenho', 'era', 'houvessem', 'tivesse', 'vos', 'estávamos', 'tu', 'foi', 'estivéramos', 'houveram', 'no', 'o', 'eu', 'esses', 'ela', 'estava', 'dela', 'te', 'as', 'estes', 'mas', 'qual', 'tive', 'houve', 'hão', 'estejamos', 'até', 'nos', 'teria', 'tivera', 'meus', 'houvera', 'aquilo', 'foram', 'houverei', 'tém', 'havemos', 'houveremos', 'meu', 'ser', 'tiverem', 'houvéramos', 'ele', 'isto', 'estivermos', 'um', 'estivéssemos', 'houvemos', 'tiveram', 'terá', 'terão', 'minhas', 'você', 'esteja', 'estejam', 'temos', 'teríamos', 'deles', 'esse', 'hajamos', 'que', 'fossem', 'estão', 'houvermos', 'estavam', 'fôramos', 'e', 'teus', 'nosso', 'pelos', 'lhe', 'são', 'vocês', 'esteve', 'uma', 'teriam', 'fôssemos', 'teremos', 'delas', 'há', 'tinha', 'estar', 'sou', 'tinham', 'estivemos', 'os', 'serão', 'do', 'à', 'nossas', 'éramos', 'será', 'tenha', 'essas', 'dos', 'em', 'muito', 'tivermos', 'tivemos', 'me', 'na', 'aqueles', 'isso', 'aquela', 'seremos', 'tuas', 'estou', 'hei', 'estivessem', 'tem', 'fosse', 'aquelas', 'tivéramos', 'para', 'tenhamos', 'houverão', 'pela', 'elas', 'houverá', 'houveriam', 'minha', 'estivera', 'numa', 'ou', 'também', 'teu', 'como', 'lhes', 'somos', 'pelo', 'a', 'estive', 'com', 'houveria', 'tiver', 'está', 'aquele', 'fora', 'seríamos'}\n",
            "\n",
            "Tokens originais: ['olá', 'este', 'é', 'um', 'exemplo', 'de', 'texto', 'com', 'várias', 'pontuações', 'símbolos', 'especiais', 'e', 'letras', 'maiúsculas'] \n",
            "Quantidade: 15\n",
            "\n",
            "Tokens sem stopwords: ['olá', 'exemplo', 'texto', 'várias', 'pontuações', 'símbolos', 'especiais', 'letras', 'maiúsculas'] \n",
            "Quantidade: 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 - Stemming e Lemalização"
      ],
      "metadata": {
        "id": "XfKaYWIssOTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Importa stemmer para redução de palavras ao radical\n",
        "from nltk.stem import RSLPStemmer\n",
        "\n",
        "# Baixa recursos para stemmer em português\n",
        "nltk.download('rslp')\n",
        "\n",
        "# Cria instância do stemmer RSLP (específico para português)\n",
        "stemmer = RSLPStemmer()\n",
        "\n",
        "# Aplica stemming em cada palavra (reduz à forma radical)\n",
        "stemming = [stemmer.stem(palavra) for palavra in tokens_sem_stopwords]\n",
        "\n",
        "# Exibe comparação entre tokens originais e seus radicais\n",
        "print('Tokens originais:', tokens_sem_stopwords)\n",
        "print('Tokens após stemming:', stemming)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdNq_wXxs64G",
        "outputId": "b237914b-2fe9-4a67-93dd-71f85d585c60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens originais: ['olá', 'exemplo', 'texto', 'várias', 'pontuações', 'símbolos', 'especiais', 'letras', 'maiúsculas']\n",
            "Tokens após stemming: ['olá', 'exempl', 'text', 'vár', 'pontu', 'símbol', 'espec', 'letr', 'maiúscul']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Exemplo 01 - Pré processamento completo\n"
      ],
      "metadata": {
        "id": "eafAzeIhtr2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa bibliotecas para processamento de linguagem natural\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import RSLPStemmer  # Stemmer específico para português\n",
        "import re\n",
        "\n",
        "# Baixa recursos necessários (executar apenas na primeira vez)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('rslp')\n",
        "\n",
        "# Recebe texto do usuário\n",
        "texto = input(\"Digite seu texto (pode conter símbolos): \")\n",
        "\n",
        "# 1. LIMPEZA DO TEXTO\n",
        "# Remove caracteres não alfabéticos e normaliza espaços\n",
        "texto_limpo = re.sub(r'[^a-zA-Zá-úÁ-Ú]', ' ', texto)\n",
        "texto_limpo = ' '.join(texto_limpo.split())  # Remove espaços extras\n",
        "\n",
        "# 2. NORMALIZAÇÃO\n",
        "# Converte para minúsculas\n",
        "texto_normalizado = texto_limpo.lower()\n",
        "\n",
        "# 3. TOKENIZAÇÃO\n",
        "# Divide o texto em palavras individuais\n",
        "tokens = nltk.word_tokenize(texto_normalizado, language='portuguese')\n",
        "\n",
        "# 4. REMOÇÃO DE STOPWORDS\n",
        "# Filtra palavras sem significado contextual\n",
        "stopwords_pt = set(stopwords.words('portuguese'))\n",
        "tokens_sem_stopwords = [p for p in tokens if p not in stopwords_pt]\n",
        "\n",
        "# 5. STEMMING (REDUÇÃO AO RADICAL)\n",
        "# Aplica stemmer específico para português\n",
        "stemmer = RSLPStemmer()\n",
        "radicais = [stemmer.stem(palavra) for palavra in tokens_sem_stopwords]\n",
        "\n",
        "# RESULTADOS\n",
        "print(\"\\nProcessamento completo:\")\n",
        "print(f\"Texto original: {texto}\")\n",
        "print(f\"Texto limpo: {texto_limpo}\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"Sem stopwords: {tokens_sem_stopwords}\")\n",
        "print(f\"Radicais: {radicais}\")\n",
        "print(f\"\\nQuantidade de palavras: {len(radicais)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3LqS7Wktwk_",
        "outputId": "4a41edf3-01c3-472b-9565-d4df5ade84ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digite seu texto (pode conter símbolos): oi\n",
            "\n",
            "Processamento completo:\n",
            "Texto original: oi\n",
            "Texto limpo: oi\n",
            "Tokens: ['oi']\n",
            "Sem stopwords: ['oi']\n",
            "Radicais: ['oi']\n",
            "\n",
            "Quantidade de palavras: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exemplo 02 - Estrutura e pré processamento de texto"
      ],
      "metadata": {
        "id": "TINoPxtRuRsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importações otimizadas para PLN\n",
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Verificação e download de recursos necessários\n",
        "try:\n",
        "    nlp = spacy.load(\"pt_core_news_sm\")\n",
        "except OSError:\n",
        "    print(\"Baixando modelo do spaCy...\")\n",
        "    !python -m spacy download pt_core_news_sm\n",
        "    nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "nltk.download(['stopwords', 'punkt_tab', 'rslp'], quiet=True)\n",
        "\n",
        "# Texto de exemplo para análise\n",
        "texto = \"\"\"\n",
        "O Processamento de Linguagem Natural (PLN) revoluciona como interagimos com tecnologia.\n",
        "Análise de sentimentos, chatbots e tradutores automáticos são aplicações comuns.\n",
        "Veja exemplos em: https://exemplo-pln.com.br! 😊\n",
        "\"\"\"\n",
        "\n",
        "# Pipeline de processamento\n",
        "def processar_texto(texto):\n",
        "    # 1. Normalização\n",
        "    texto = re.sub(r'https?://\\S+|www\\.\\S+', '', texto)  # Remove URLs\n",
        "    texto = re.sub(r'[^\\w\\sá-úÁ-Ú]', '', texto.lower())  # Padronização\n",
        "\n",
        "    # 2. Tokenização\n",
        "    tokens = nltk.word_tokenize(texto, language='portuguese')\n",
        "\n",
        "    # 3. Filtragem\n",
        "    stopwords_pt = set(stopwords.words('portuguese'))\n",
        "    tokens_filtrados = [t for t in tokens if t not in stopwords_pt and len(t) > 2]\n",
        "\n",
        "    # 4. Stemming\n",
        "    stemmer = nltk.RSLPStemmer()\n",
        "    stems = [stemmer.stem(t) for t in tokens_filtrados]\n",
        "\n",
        "    # 5. Lematização\n",
        "    lemas = [t.lemma_ for t in nlp(\" \".join(tokens_filtrados))]\n",
        "\n",
        "    return {\n",
        "        'original': texto,\n",
        "        'tokens': tokens,\n",
        "        'filtrados': tokens_filtrados,\n",
        "        'stems': stems,\n",
        "        'lemas': lemas\n",
        "    }\n",
        "\n",
        "# Processamento e exibição\n",
        "resultado = processar_texto(texto)\n",
        "print(f\"\\nTexto original:\\n{texto}\")\n",
        "print(f\"\\nTokens ({len(resultado['tokens'])}):\\n{resultado['tokens']}\")\n",
        "print(f\"\\nSem stopwords ({len(resultado['filtrados'])}):\\n{resultado['filtrados']}\")\n",
        "print(f\"\\nStems ({len(resultado['stems'])}):\\n{resultado['stems']}\")\n",
        "print(f\"\\nLematização ({len(resultado['lemas'])}):\\n{resultado['lemas']}\")\n",
        "\n",
        "# Análise comparativa\n",
        "diferencas = sum(1 for s,l in zip(resultado['stems'], resultado['lemas']) if s != l)\n",
        "print(f\"\\nDiferenças entre stemming/lematização: {diferencas}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BPaWgS0uYAT",
        "outputId": "110bc3f9-4d02-4227-d1da-5f2079bad2fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baixando modelo do spaCy...\n",
            "Collecting pt-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pt-core-news-sm\n",
            "Successfully installed pt-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "\n",
            "Texto original:\n",
            "\n",
            "O Processamento de Linguagem Natural (PLN) revoluciona como interagimos com tecnologia.\n",
            "Análise de sentimentos, chatbots e tradutores automáticos são aplicações comuns. \n",
            "Veja exemplos em: https://exemplo-pln.com.br! 😊\n",
            "\n",
            "\n",
            "Tokens (24):\n",
            "['o', 'processamento', 'de', 'linguagem', 'natural', 'pln', 'revoluciona', 'como', 'interagimos', 'com', 'tecnologia', 'análise', 'de', 'sentimentos', 'chatbots', 'e', 'tradutores', 'automáticos', 'são', 'aplicações', 'comuns', 'veja', 'exemplos', 'em']\n",
            "\n",
            "Sem stopwords (16):\n",
            "['processamento', 'linguagem', 'natural', 'pln', 'revoluciona', 'interagimos', 'tecnologia', 'análise', 'sentimentos', 'chatbots', 'tradutores', 'automáticos', 'aplicações', 'comuns', 'veja', 'exemplos']\n",
            "\n",
            "Stems (16):\n",
            "['process', 'lingu', 'natur', 'pln', 'revoluc', 'interag', 'tecnolog', 'anális', 'sent', 'chatbot', 'tradu', 'automá', 'aplic', 'comum', 'vej', 'exempl']\n",
            "\n",
            "Lematização (16):\n",
            "['processamento', 'linguagem', 'natural', 'pln', 'revolucionar', 'interagimos', 'tecnologia', 'análiser', 'sentimento', 'chatbots', 'tradutor', 'automático', 'aplicação', 'comum', 'ver', 'exemplo']\n",
            "\n",
            "Diferenças entre stemming/lematização: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exemplo 03 - Modelo pré treinado\n"
      ],
      "metadata": {
        "id": "txmhzCC1-MRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Importa a biblioteca spaCy para processamento de linguagem natural\n",
        "import spacy\n",
        "from spacy import displacy  # Para visualização de dependências\n",
        "\n",
        "# Carrega o modelo pré-treinado para português\n",
        "# (Certifique-se de ter executado antes: !python -m spacy download pt_core_news_sm)\n",
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "# Recebe texto do usuário para análise\n",
        "texto = input(\"Digite um texto para análise linguística: \")\n",
        "\n",
        "# Processa o texto com o pipeline do spaCy\n",
        "doc = nlp(texto)\n",
        "\n",
        "# 1. ANÁLISE MORFOSSINTÁTICA (Part-of-Speech Tagging)\n",
        "print('\\n🔠 ANÁLISE GRAMATICAL:')\n",
        "for token in doc:\n",
        "    print(f\"Palavra: {token.text:<15} | Classe gramatical: {token.pos_:<10} | Tag detalhada: {token.tag_}\")\n",
        "\n",
        "# 2. ANÁLISE DE DEPENDÊNCIAS\n",
        "print(\"\\n🔗 ANÁLISE DE DEPENDÊNCIAS:\")\n",
        "for token in doc:\n",
        "    print(f\"Palavra: {token.text:<15} | Relação: {token.dep_:<10} | Governante: {token.head.text}\")\n",
        "\n",
        "# 3. VISUALIZAÇÃO GRÁFICA (opcional - funciona no Jupyter/Colab)\n",
        "print(\"\\n🌳 VISUALIZAÇÃO DA ÁRVORE DE DEPENDÊNCIAS:\")\n",
        "displacy.render(doc, style=\"dep\", jupyter=True, options={'distance': 90})\n",
        "\n",
        "# 4. INFORMAÇÕES ADICIONAIS\n",
        "print(\"\\n📊 ESTATÍSTICAS DO TEXTO:\")\n",
        "print(f\"Número de tokens: {len(doc)}\")\n",
        "print(f\"Número de frases: {len(list(doc.sents))}\")\n",
        "print(f\"Palavras únicas: {len(set(token.text for token in doc))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "UHpsxXo3-ekM",
        "outputId": "edb935bb-d752-4e49-f1c1-d42d372def30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digite um texto para análise linguística: ola! tudo bem?\n",
            "\n",
            "🔠 ANÁLISE GRAMATICAL:\n",
            "Palavra: ola             | Classe gramatical: INTJ       | Tag detalhada: INTJ\n",
            "Palavra: !               | Classe gramatical: PUNCT      | Tag detalhada: PUNCT\n",
            "Palavra: tudo            | Classe gramatical: PRON       | Tag detalhada: PRON\n",
            "Palavra: bem             | Classe gramatical: ADV        | Tag detalhada: ADV\n",
            "Palavra: ?               | Classe gramatical: PUNCT      | Tag detalhada: PUNCT\n",
            "\n",
            "🔗 ANÁLISE DE DEPENDÊNCIAS:\n",
            "Palavra: ola             | Relação: ROOT       | Governante: ola\n",
            "Palavra: !               | Relação: punct      | Governante: ola\n",
            "Palavra: tudo            | Relação: ROOT       | Governante: tudo\n",
            "Palavra: bem             | Relação: advmod     | Governante: tudo\n",
            "Palavra: ?               | Relação: punct      | Governante: tudo\n",
            "\n",
            "🌳 VISUALIZAÇÃO DA ÁRVORE DE DEPENDÊNCIAS:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"pt\" id=\"12e12ac27b7f4b079fbfe6a1392ad1e0-0\" class=\"displacy\" width=\"320\" height=\"182.0\" direction=\"ltr\" style=\"max-width: none; height: 182.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"92.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">ola!</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">INTJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"92.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">tudo</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"92.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">bem?</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">ADV</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-12e12ac27b7f4b079fbfe6a1392ad1e0-0-0\" stroke-width=\"2px\" d=\"M160,47.0 C160,2.0 230.0,2.0 230.0,47.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-12e12ac27b7f4b079fbfe6a1392ad1e0-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M230.0,49.0 L238.0,37.0 222.0,37.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 ESTATÍSTICAS DO TEXTO:\n",
            "Número de tokens: 5\n",
            "Número de frases: 2\n",
            "Palavras únicas: 5\n"
          ]
        }
      ]
    }
  ]
}
